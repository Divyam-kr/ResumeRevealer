{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:15:25.702344600Z",
     "start_time": "2024-02-29T18:15:22.653105800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pinecone\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:19<00:00,  1.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = r'C:\\Users\\jaini\\PycharmProjects\\pythonProject3\\documents\\pdf'\n",
    "\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory,show_progress=True) #unstructuredloader by default has used this auto identify file type and load it, mode=\"single\", strategy='fast'(other option is strategy='hi_res' that use yolo varient if mode is elements)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory) \n",
    "len(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:15:50.026224500Z",
     "start_time": "2024-02-29T18:15:30.054080800Z"
    }
   },
   "id": "811cb22676994805"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Document(page_content='Maaz Muhammad Khan\\n\\n\\uf020maazkhan1211@gmail.com | (216)-666-095|\\n\\nEducation\\n\\nM.S. Computer Science (GPA 3.77) January 2020 –December 2021 Cleveland State University\\n\\nRelevant Coursework: Data Mining, Big data, Machine Learning, Artificial Intelligence, Data communication and Networks\\n\\nB.S. Computer Science (GPA 3.39) January 2015 – December 2018\\n\\nMuhammad Ali Jinnah University\\n\\nRelevant Coursework: Database Systems, Cloud Computing, Statistics, Algorithms and Data Structures, Object Oriented Programming, Human Computer Interaction, Web Development, Computer Architecture, Software Requirement Engineering, Software Quality Assurance\\n\\nWork Experience\\n\\nBusiness Intelligence Analyst\\\\Data Engineer Jan 2022-Present New York Air Brake, Watertown, NY • Designed & developed visual BI reports and dashboards to support management decision-making using Power bi and Tableau. • Developed and automated data pipelines and ETL process and deployed new data models to make datasets readily-consumable by visualization tools\\n\\nusing Snowflake and SQL Server.\\n\\nDrove efficiencies throughout the organization by automating business processes. • Gathered and analyzed data from multiple sources to solve complex problems and prepared summaries of analysis and findings to management team. • Worked with leadership to understand business needs and translate those into reporting solutions. • • Developed and maintain data quality standards and data governance policies. Systems Administrator May 2019 – Jan 2022 Lansing Board of Water and Light, Lansing, MI • Designed and developed dashboards which helped managers in making critical decisions to drive employee productivity and resource utilization. • Created dashboards with context and overlayed analytics and live, interactive, reports that easily compare and analyze assets using Microsoft Power-Bi • Managed server, data repositories and the real-time data storage, normalization, analytics, and notification engine using Microsoft Azure SQL database. • Summarized and filtered years of historical data and view alongside real-time data which allowed summary calculations • Used python and SQL for ETL processes data cleansing, normalization, and transformation for operations analytics project. • Also used Microsoft Azure DevOps for version control, reporting, requirements management, project management, automated builds, testing and release\\n\\nmanagement capabilities for the entire application lifecycle(Agile Methodology).\\n\\nWorked on real-time data from grid and network devices for data analytics and applied machine learning to automate network response. Research Assistant Feb 2020 – April 2021 Artificial Intelligence and Computer Vision Lab, Cleveland State University, Cleveland, OH. • • • • • • Applied machine learning models like, CNN, KNN, Kmeans, SVM, Naïve Bayes, Regression, Gradient Boosting Algorithms • Determined the hidden relation between images and text using text and image mining libraries Nltk and Gensim • Worked on image recognition using Tensorflow and Keras for implementation of (CNN) with different initializers like Adam and SGD. Data analyst/Enginer May 2016 – July 2020 John Carroll University, Cleveland, OH • Assisted data scientists in building sentiment analysis and Used python and SQL for text mining and artificial intelligence techniques. • • Analyzed complex data sets and translated data into insights to drive key business decisions using Tableau and Microsoft Power BI. • • Mined huge textual dataset of online reviews of different movies using NLTk and Genism and Categorized data into POS, removed stop words • BI tools: Tableau, Microsoft Power BI, Power Query, DAX SAP Analytics Looker, Google Charts, Qlik, SSRS, SSIS\\n\\nWorked on real-time data from grid and network devices for data analytics and applied machine learning to automate network response. Research Assistant Feb 2020 – April 2021 Artificial Intelligence and Computer Vision Lab, Cleveland State University, Cleveland, OH. • • • • • • Applied machine learning models like, CNN, KNN, Kmeans, SVM, Naïve Bayes, Regression, Gradient Boosting Algorithms • Determined the hidden relation between images and text using text and image mining libraries Nltk and Gensim • Worked on image recognition using Tensorflow and Keras for implementation of (CNN) with different initializers like Adam and SGD. Data analyst/Enginer May 2016 – July 2020 John Carroll University, Cleveland, OH • Assisted data scientists in building sentiment analysis and Used python and SQL for text mining and artificial intelligence techniques. • • Analyzed complex data sets and translated data into insights to drive key business decisions using Tableau and Microsoft Power BI. • • Mined huge textual dataset of online reviews of different movies using NLTk and Genism and Categorized data into POS, removed stop words • BI tools: Tableau, Microsoft Power BI, Power Query, DAX SAP Analytics Looker, Google Charts, Qlik, SSRS, SSIS\\n\\nCollected dataset by web scraping techniques using libraries like selenium, beautiful soup, and response. Stored unstructured using Pymongo and did preprocessing/cleansing using Sk.learn and Opencv, numpy, pandas , skiamge Sliced data using Sklearn.model_selection for testing and training and added layers filters and activation to convolutional neural networks. Transformed data and developed a system which classifies images into subcategories with the help of cnn using Pytorch & Torchvision Extracted features from datasets using ICA, LBP, Hog, PCA , LDA, LLE, AE , and etc.\\n\\nAnalytic Development: Python, R-programming, MATLAB, SQL, C, C++, C#, HTML, CSS, Java, XML • • Cloud Services: AWS, Azure and Google Cloud, Snowflake, Informatica • Miscellaneous Tools: SAP SAC, JIRA, Azure DevOps, SharePoint MS-Teams. • Artificial Intelligence & Analysis Methods:: Text comprehension, NLP, Classification, Pattern Recognition, Imaging Recognition and Detection, forecasting, clustering, Sentiment Analysis, Predictive Analytics, Decision Analytics, Association Analysis, Attribution modeling, Classification and Regression Trees (CART), SVM, Random Forest, GBM,, PCA, RNN, Regression, Naïve Bayes\\n\\nData Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling • Python Packages: NumPy, Pandas, Scikit-learn, TensorFlow, SciPy, Matplotlib, Seaborn, Keras, NLTK, Beautiful Soup, SciPy, StatsModels, PyTorch, OpenCV, Sweetviz, Theano, Bokeh, Plotly, Genism, Scrapy, kivy, Caffe2 • IDE & Version Control: Jupyter Notebook, Spyder, RStudio, Visual Studio, SharePoint, A/B testing GitHub, Git, Agile methodologies • Databases: Oracle, MS SQL Server, Snowflake, MS Access Amazon S3, Azure, Mongo DB , My SQL, PostgreSQL and Hadoop, Pyspark . • Soft Skills: Communication and presentation skills; teamwork; leadership, critical thinking, creativity, and problem-solving, accountability.', metadata={'source': 'C:\\\\Users\\\\jaini\\\\PycharmProjects\\\\pythonProject3\\\\documents\\\\pdf\\\\Maaz khan _ d17.pdf'})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[6]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:15:57.857345600Z",
     "start_time": "2024-02-29T18:15:57.844260800Z"
    }
   },
   "id": "457834c3fa903ceb"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# def split_docs(documents, chunk_size=1000, chunk_overlap=100):\n",
    "#   text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#   docs = text_splitter.split_documents(documents)\n",
    "#   return docs\n",
    "# \n",
    "# docs = split_docs(documents)\n",
    "# print(len(docs))\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=\"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\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:16:06.683809900Z",
     "start_time": "2024-02-29T18:16:06.591816900Z"
    }
   },
   "id": "9f62e80bc5f518e1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "\n",
    "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\" , dimensions=384)\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=documents, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)\n",
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None\n",
    "\n",
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding)\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3}) # by default search_type=\"similarity_score_threshold\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:17:12.100885900Z",
     "start_time": "2024-02-29T18:17:06.691538500Z"
    }
   },
   "id": "42c1e3b6e1864785"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000018BBE523D30>, search_kwargs={'k': 3})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:17:13.892057700Z",
     "start_time": "2024-02-29T18:17:13.871373700Z"
    }
   },
   "id": "219f7012142b3f19"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"which candidate is good fit for Machine learning engineer roles.\")\n",
    "\n",
    "\n",
    "print(len(docs))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:17:15.213754800Z",
     "start_time": "2024-02-29T18:17:14.786879500Z"
    }
   },
   "id": "533a5a8958dba79b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Worked on real-time data from grid and network devices for data analytics and applied machine learning to automate network response. Research Assistant Feb 2020 – April 2021 Artificial Intelligence and Computer Vision Lab, Cleveland State University, Cleveland, OH. • • • • • • Applied machine learning models like, CNN, KNN, Kmeans, SVM, Naïve Bayes, Regression, Gradient Boosting Algorithms • Determined the hidden relation between images and text using text and image mining libraries Nltk and Gensim • Worked on image recognition using Tensorflow and Keras for implementation of (CNN) with different initializers like Adam and SGD. Data analyst/Enginer May 2016 – July 2020 John Carroll University, Cleveland, OH • Assisted data scientists in building sentiment analysis and Used python and SQL for text mining and artificial intelligence techniques. • • Analyzed complex data sets and translated data into insights to drive key business decisions using Tableau and Microsoft Power BI. • • Mined' metadata={'source': 'C:\\\\Users\\\\jaini\\\\PycharmProjects\\\\pythonProject3\\\\documents\\\\pdf\\\\Maaz khan _ d17.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:17:41.101408600Z",
     "start_time": "2024-02-29T18:17:41.089398700Z"
    }
   },
   "id": "8ceb4580d936d47"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "Document(page_content='Worked on real-time data from grid and network devices for data analytics and applied machine learning to automate network response. Research Assistant Feb 2020 – April 2021 Artificial Intelligence and Computer Vision Lab, Cleveland State University, Cleveland, OH. • • • • • • Applied machine learning models like, CNN, KNN, Kmeans, SVM, Naïve Bayes, Regression, Gradient Boosting Algorithms • Determined the hidden relation between images and text using text and image mining libraries Nltk and Gensim • Worked on image recognition using Tensorflow and Keras for implementation of (CNN) with different initializers like Adam and SGD. Data analyst/Enginer May 2016 – July 2020 John Carroll University, Cleveland, OH • Assisted data scientists in building sentiment analysis and Used python and SQL for text mining and artificial intelligence techniques. • • Analyzed complex data sets and translated data into insights to drive key business decisions using Tableau and Microsoft Power BI. • • Mined', metadata={'source': 'C:\\\\Users\\\\jaini\\\\PycharmProjects\\\\pythonProject3\\\\documents\\\\pdf\\\\Maaz khan _ d17.pdf'})"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T17:46:49.380812700Z",
     "start_time": "2024-02-29T17:46:49.352813400Z"
    }
   },
   "id": "bc0ee97c9dbff528"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDUCATION\n",
      "\n",
      "New York University, New York City, NY\n",
      "\n",
      "Aug 2022-May 2024\n",
      "\n",
      "Master of Science in Data Science: Introduction to Data Science, Probability & Statistics, Optimization & Computational Linear Algebra\n",
      "\n",
      "Veermata Jijabai Technological Institute (VJTI), Mumbai, India\n",
      "\n",
      "Aug 2017-May 2021\n",
      "\n",
      "Bachelor of Technology in Electrical Engineering; Projects in Machine Learning and Deep Learning GPA: 8.47 / 10\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Business Analyst, Axis Bank , Mumbai, India ● Created and maintained the ETB Personal Loan XSell Dashboard on SAS Viya on a monthly basis by performing data analysis using\n",
      "\n",
      "Aug 2021-Jul 2022\n",
      "\n",
      "SAS and SQL to present key metrics pertaining to business overview, conversion rates, base synthesis and risk of the ETB Personal Loan business to various business stakeholders\n",
      "\n",
      "Prepared the Retail Asset without Saving Account Pre Qualified customer database for Personal Loan by extracting,merging and filtering data from source tables on a monthly basis\n"
     ]
    }
   ],
   "source": [
    "# # docs = retriever.invoke(\"which candidate is good fit for Data Analyst roles.\")\n",
    "# docs = retriever.invoke(\"Give name of candidate who is good fit for Data Analyst roles.\")\n",
    "# \n",
    "# print(docs[0].page_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T17:24:53.225706900Z",
     "start_time": "2024-02-29T17:24:52.744834900Z"
    }
   },
   "id": "5756f0661d515094"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "# create the chain to answer questions \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)\n",
    "## Cite sources\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T17:26:52.815303600Z",
     "start_time": "2024-02-29T17:26:52.778475900Z"
    }
   },
   "id": "40d79ef6ebb89450"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaini\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have the specific information needed to provide a shortlisted resume based on the job description you provided.\n",
      "\n",
      "\n",
      "Sources:\n",
      "C:\\Users\\jaini\\PycharmProjects\\pythonProject3\\documents\\pdf\\NYU CDS-Revelio Labs Resume Book_Fall 2022 Career Fair (Internship).pdf\n",
      "C:\\Users\\jaini\\PycharmProjects\\pythonProject3\\documents\\pdf\\NYU CDS-Revelio Labs Resume Book_Fall 2022 Career Fair (Full-Time).pdf\n"
     ]
    }
   ],
   "source": [
    "# full example\n",
    "warning = \"If you don't know the answer, just say that you don't know, don't try to make up an answer\"\n",
    "job_description = \"MS in computer science or a related technical field,5+ years of industry work experience. Good sense of product with a focus on shipping user-facing data-driven features, Expertise in Python and Python based ML/DL and Data Science frameworks. \\\n",
    "Excellent coding, analysis, and problem-solving skills. Proven knowledge of data structure and algorithms. \\\n",
    "Familiarity in relevant machine learning frameworks and packages such as Tensorflow, PyTorch and HuggingFace\\\n",
    "Experience working with Product Management and decomposing feature requirements into technical work items to ship products\\\n",
    "Experience with generative AI, knowledge of ML Ops and ML services is a plus. This includes Pinecone, LangChain, Weights and Biases etc. \\\n",
    "Familiarity with deployment technologies such as Docker, Kubernetes and Triton are a plus\\\n",
    "Strong communication and collaboration skills\"\n",
    "question = warning+job_description + \" Based on the given job description\"\n",
    "query = question + \"short list resumes which is good fit based on skills,education and work experience mentioned in it? also provide the candidate name which will be mentioned in first line of pdf without subheading\"\n",
    "# query = \"short list resumes which is good fit for Data analysis roles based on skills,education and work experience mwntioned in it?\"\n",
    "\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T17:27:05.530532700Z",
     "start_time": "2024-02-29T17:27:03.516909500Z"
    }
   },
   "id": "5987d56d09aba2a8"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have the specific resumes of candidates to provide a shortlist based on the given job description.\n",
      "\n",
      "\n",
      "Sources:\n",
      "C:\\Users\\jaini\\PycharmProjects\\pythonProject3\\documents\\pdf\\NYU CDS-Revelio Labs Resume Book_Fall 2022 Career Fair (Internship).pdf\n",
      "C:\\Users\\jaini\\PycharmProjects\\pythonProject3\\documents\\pdf\\NYU CDS-Revelio Labs Resume Book_Fall 2022 Career Fair (Full-Time).pdf\n"
     ]
    }
   ],
   "source": [
    "# full example\n",
    "warning = \"If you don't know the answer, just say that you don't know, don't try to make up an answer\"\n",
    "job_description = \"MS in computer science or a related technical field,5+ years of industry work experience. Good sense of product with a focus on shipping user-facing data-driven features, Expertise in Python and Python based ML/DL and Data Science frameworks. \\\n",
    "Excellent coding, analysis, and problem-solving skills. Proven knowledge of data structure and algorithms. \\\n",
    "Familiarity in relevant machine learning frameworks and packages such as Tensorflow, PyTorch and HuggingFace\\\n",
    "Experience working with Product Management and decomposing feature requirements into technical work items to ship products\\\n",
    "Experience with generative AI, knowledge of ML Ops and ML services is a plus. This includes Pinecone, LangChain, Weights and Biases etc. \\\n",
    "Familiarity with deployment technologies such as Docker, Kubernetes and Triton are a plus\\\n",
    "Strong communication and collaboration skills\"\n",
    "question = warning+job_description + \" Based on the given job description\"\n",
    "query = question + \"short list resumes which is good fit based on skills,education and work experience mwntioned in it? also provide the candidate name which will be mentioned in first line of pdf without subheading\"\n",
    "# query = \"short list resumes which is good fit for Data analysis roles based on skills,education and work experience mwntioned in it?\"\n",
    "\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T17:27:29.595733100Z",
     "start_time": "2024-02-29T17:27:28.138426600Z"
    }
   },
   "id": "c6a102ff50914dc1"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='EDUCATION\\n\\nNew York University, Center for Data Science\\n\\nJune 2024\\n\\nMasters in Data Science\\n\\nNational Tsing Hua University, Taiwan Bachelor of Quantitative Finance Elective courses in data science\\n\\nJune 2021\\n\\nSKILLS\\n\\nProgramming: Python (Intermediate) (GitHub), SQL (Expert), MATLAB Technical Application: PCA, Regression, Optimization, Gradient Descent, Machine Learning, Text-Mining,\\n\\nWeb Crawling, Data Visualization, Corporate Finance, Derivative Pricing\\n\\nEXPERIENCE\\n\\nPinkoi, Data Scientist Intern (Full-time)\\n\\nJuly 2021 – Feb 2022\\n\\nAnalyzed and modeled (Regression and Machine Learning methods) quantitative and qualitative data in AWS database for user behavior research\\n\\nConducted experiment A/B testing, and product optimization for e-commerce web\\\\app platforms ● Constructed user road map to improve understanding of different phases of user behavior and increase user conversion rate', metadata={'source': 'C:\\\\Users\\\\jaini\\\\PycharmProjects\\\\pythonProject3\\\\documents\\\\pdf\\\\NYU CDS-Revelio Labs Resume Book_Fall 2022 Career Fair (Internship).pdf'}), Document(page_content='Chicago, IL December 2018 - March 2020 June 2017 - December 2018\\n\\n\\n\\n●\\n\\n\\n\\n\\n\\nDeveloped PySpark code that met source to target requirements and created ETL pipelines, unit tested ETL code, and strengthened platform’s common functions in code base Analyzed various industry’s datasets by cleaning, transforming, and modeling (i.e. linear models) Utilized the following technologies: Hadoop ecosystem to manage file system, Microsoft Azure Cloud and TFS to build and release code, and SQL to validate data Documented best practices relating to useful Hadoop/Git commands, Spark error handling, a creative Git branching strategy, effective Python functions, and general knowledge transfer Conducted several stakeholder interviews to identify data consumption needs of the client and proposed appropriate solutions to sign new statements of work Consulted on the following clients: insurance, healthcare insurance, financial services\\n\\n\\n\\nZillow/NYU Capstone Graduate Research Scientist', metadata={'source': 'C:\\\\Users\\\\jaini\\\\PycharmProjects\\\\pythonProject3\\\\documents\\\\pdf\\\\NYU CDS-Revelio Labs Resume Book_Fall 2022 Career Fair (Full-Time).pdf'})]\n"
     ]
    }
   ],
   "source": [
    "warning = \"If you don't know the answer, just say that you don't know, don't try to make up an answer\"\n",
    "job_description = \"MS in computer science or a related technical field,5+ years of industry work experience. Good sense of product with a focus on shipping user-facing data-driven features, Expertise in Python and Python based ML/DL and Data Science frameworks. \\\n",
    "Excellent coding, analysis, and problem-solving skills. Proven knowledge of data structure and algorithms. \\\n",
    "Familiarity in relevant machine learning frameworks and packages such as Tensorflow, PyTorch and HuggingFace\\\n",
    "Experience working with Product Management and decomposing feature requirements into technical work items to ship products\\\n",
    "Experience with generative AI, knowledge of ML Ops and ML services is a plus. This includes Pinecone, LangChain, Weights and Biases etc. \\\n",
    "Familiarity with deployment technologies such as Docker, Kubernetes and Triton are a plus\\\n",
    "Strong communication and collaboration skills\"\n",
    "question = warning+job_description + \" Based on the given job description\"\n",
    "query = question + \"retrive the full document information of a resume which is good fit based on skills,education and work experience mwntioned in it? \"\n",
    "\n",
    "resume_doc = retriever.invoke(query)\n",
    "\n",
    "print(resume_doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T17:27:36.496683500Z",
     "start_time": "2024-02-29T17:27:35.786987700Z"
    }
   },
   "id": "d599e9f86ba4224a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDUCATION\n",
      "\n",
      "New York University, Center for Data Science\n",
      "\n",
      "June 2024\n",
      "\n",
      "Masters in Data Science\n",
      "\n",
      "National Tsing Hua University, Taiwan Bachelor of Quantitative Finance Elective courses in data science\n",
      "\n",
      "June 2021\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Programming: Python (Intermediate) (GitHub), SQL (Expert), MATLAB Technical Application: PCA, Regression, Optimization, Gradient Descent, Machine Learning, Text-Mining,\n",
      "\n",
      "Web Crawling, Data Visualization, Corporate Finance, Derivative Pricing\n",
      "\n",
      "EXPERIENCE\n",
      "\n",
      "Pinkoi, Data Scientist Intern (Full-time)\n",
      "\n",
      "July 2021 – Feb 2022\n",
      "\n",
      "Analyzed and modeled (Regression and Machine Learning methods) quantitative and qualitative data in AWS database for user behavior research\n",
      "\n",
      "Conducted experiment A/B testing, and product optimization for e-commerce web\\app platforms ● Constructed user road map to improve understanding of different phases of user behavior and increase user conversion rate\n"
     ]
    }
   ],
   "source": [
    "resume_doc = resume_doc[0].page_content\n",
    "print(resume_doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T17:28:02.658906300Z",
     "start_time": "2024-02-29T17:28:02.646905500Z"
    }
   },
   "id": "6c3144ad19457172"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resume_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mresume_doc\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'resume_doc' is not defined"
     ]
    }
   ],
   "source": [
    "resume_doc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:24:43.935292300Z",
     "start_time": "2024-02-29T18:24:42.169847400Z"
    }
   },
   "id": "383419898e447008"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\nSkills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\nEducation: What is the highest education of the candidate and what is the GPA as mentioned in the text?Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\\n\\nProjects: Extract all project titles mentioned in a textand output them as a comma separated Python list.\\n\\nPublications: Extract all publication titles mentioned in a textand output them as a comma separated Python list.\\n\\nWork experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\nSkills\\nEducation\\nProjects\\nPublications\\nWork experience\\n\\ntext: {text}\\n'))]\n"
     ]
    }
   ],
   "source": [
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Skills: what are the technical and non technical skills? \\\n",
    "Answer output them as a comma separated Python list.\n",
    "\n",
    "Education: What is the highest education of the candidate and what is the GPA as mentioned in the text?\\\n",
    "Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\n",
    "\n",
    "Projects: Extract all project titles mentioned in a text\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Publications: Extract all publication titles mentioned in a text\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Work experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designation\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "Skills\n",
    "Education\n",
    "Projects\n",
    "Publications\n",
    "Work experience\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:25:42.538180600Z",
     "start_time": "2024-02-29T18:25:42.463181400Z"
    }
   },
   "id": "752487b7f0a437bd"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\nSkills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\n\\nFormat the output as JSON with the following keys:\\nSkills\\n\\n\\ntext: {text}\\n'))]\n"
     ]
    }
   ],
   "source": [
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Skills: what are the technical and non technical skills? \\\n",
    "Answer output them as a comma separated Python list.\n",
    "\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "Skills\n",
    "\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:26:36.101932Z",
     "start_time": "2024-02-29T18:26:36.086927300Z"
    }
   },
   "id": "d2f9fec5481f3ec4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Skills"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b2678c0bf5dc0f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: [HumanMessage(content='For the following text, extract the following information:\\n\\nSkills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\n\\nFormat the output as JSON with the following keys:\\nSkills\\n\\n\\ntext: Maaz Muhammad Khan\\n\\n\\uf020maazkhan1211@gmail.com | (216)-666-095|\\n\\nEducation\\n\\nM.S. Computer Science (GPA 3.77) January 2020 –December 2021 Cleveland State University\\n\\nRelevant Coursework: Data Mining, Big data, Machine Learning, Artificial Intelligence, Data communication and Networks\\n\\nB.S. Computer Science (GPA 3.39) January 2015 – December 2018\\n\\nMuhammad Ali Jinnah University\\n\\nRelevant Coursework: Database Systems, Cloud Computing, Statistics, Algorithms and Data Structures, Object Oriented Programming, Human Computer Interaction, Web Development, Computer Architecture, Software Requirement Engineering, Software Quality Assurance\\n\\nWork Experience\\n\\nBusiness Intelligence Analyst\\\\Data Engineer Jan 2022-Present New York Air Brake, Watertown, NY • Designed & developed visual BI reports and dashboards to support management decision-making using Power bi and Tableau. • Developed and automated data pipelines and ETL process and deployed new data models to make datasets readily-consumable by visualization tools\\n\\nusing Snowflake and SQL Server.\\n\\nDrove efficiencies throughout the organization by automating business processes. • Gathered and analyzed data from multiple sources to solve complex problems and prepared summaries of analysis and findings to management team. • Worked with leadership to understand business needs and translate those into reporting solutions. • • Developed and maintain data quality standards and data governance policies. Systems Administrator May 2019 – Jan 2022 Lansing Board of Water and Light, Lansing, MI • Designed and developed dashboards which helped managers in making critical decisions to drive employee productivity and resource utilization. • Created dashboards with context and overlayed analytics and live, interactive, reports that easily compare and analyze assets using Microsoft Power-Bi • Managed server, data repositories and the real-time data storage, normalization, analytics, and notification engine using Microsoft Azure SQL database. • Summarized and filtered years of historical data and view alongside real-time data which allowed summary calculations • Used python and SQL for ETL processes data cleansing, normalization, and transformation for operations analytics project. • Also used Microsoft Azure DevOps for version control, reporting, requirements management, project management, automated builds, testing and release\\n\\nmanagement capabilities for the entire application lifecycle(Agile Methodology).\\n\\nWorked on real-time data from grid and network devices for data analytics and applied machine learning to automate network response. Research Assistant Feb 2020 – April 2021 Artificial Intelligence and Computer Vision Lab, Cleveland State University, Cleveland, OH. • • • • • • Applied machine learning models like, CNN, KNN, Kmeans, SVM, Naïve Bayes, Regression, Gradient Boosting Algorithms • Determined the hidden relation between images and text using text and image mining libraries Nltk and Gensim • Worked on image recognition using Tensorflow and Keras for implementation of (CNN) with different initializers like Adam and SGD. Data analyst/Enginer May 2016 – July 2020 John Carroll University, Cleveland, OH • Assisted data scientists in building sentiment analysis and Used python and SQL for text mining and artificial intelligence techniques. • • Analyzed complex data sets and translated data into insights to drive key business decisions using Tableau and Microsoft Power BI. • • Mined huge textual dataset of online reviews of different movies using NLTk and Genism and Categorized data into POS, removed stop words • BI tools: Tableau, Microsoft Power BI, Power Query, DAX SAP Analytics Looker, Google Charts, Qlik, SSRS, SSIS\\n\\nWorked on real-time data from grid and network devices for data analytics and applied machine learning to automate network response. Research Assistant Feb 2020 – April 2021 Artificial Intelligence and Computer Vision Lab, Cleveland State University, Cleveland, OH. • • • • • • Applied machine learning models like, CNN, KNN, Kmeans, SVM, Naïve Bayes, Regression, Gradient Boosting Algorithms • Determined the hidden relation between images and text using text and image mining libraries Nltk and Gensim • Worked on image recognition using Tensorflow and Keras for implementation of (CNN) with different initializers like Adam and SGD. Data analyst/Enginer May 2016 – July 2020 John Carroll University, Cleveland, OH • Assisted data scientists in building sentiment analysis and Used python and SQL for text mining and artificial intelligence techniques. • • Analyzed complex data sets and translated data into insights to drive key business decisions using Tableau and Microsoft Power BI. • • Mined huge textual dataset of online reviews of different movies using NLTk and Genism and Categorized data into POS, removed stop words • BI tools: Tableau, Microsoft Power BI, Power Query, DAX SAP Analytics Looker, Google Charts, Qlik, SSRS, SSIS\\n\\nCollected dataset by web scraping techniques using libraries like selenium, beautiful soup, and response. Stored unstructured using Pymongo and did preprocessing/cleansing using Sk.learn and Opencv, numpy, pandas , skiamge Sliced data using Sklearn.model_selection for testing and training and added layers filters and activation to convolutional neural networks. Transformed data and developed a system which classifies images into subcategories with the help of cnn using Pytorch & Torchvision Extracted features from datasets using ICA, LBP, Hog, PCA , LDA, LLE, AE , and etc.\\n\\nAnalytic Development: Python, R-programming, MATLAB, SQL, C, C++, C#, HTML, CSS, Java, XML • • Cloud Services: AWS, Azure and Google Cloud, Snowflake, Informatica • Miscellaneous Tools: SAP SAC, JIRA, Azure DevOps, SharePoint MS-Teams. • Artificial Intelligence & Analysis Methods:: Text comprehension, NLP, Classification, Pattern Recognition, Imaging Recognition and Detection, forecasting, clustering, Sentiment Analysis, Predictive Analytics, Decision Analytics, Association Analysis, Attribution modeling, Classification and Regression Trees (CART), SVM, Random Forest, GBM,, PCA, RNN, Regression, Naïve Bayes\\n\\nData Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling • Python Packages: NumPy, Pandas, Scikit-learn, TensorFlow, SciPy, Matplotlib, Seaborn, Keras, NLTK, Beautiful Soup, SciPy, StatsModels, PyTorch, OpenCV, Sweetviz, Theano, Bokeh, Plotly, Genism, Scrapy, kivy, Caffe2 • IDE & Version Control: Jupyter Notebook, Spyder, RStudio, Visual Studio, SharePoint, A/B testing GitHub, Git, Agile methodologies • Databases: Oracle, MS SQL Server, Snowflake, MS Access Amazon S3, Azure, Mongo DB , My SQL, PostgreSQL and Hadoop, Pyspark . • Soft Skills: Communication and presentation skills; teamwork; leadership, critical thinking, creativity, and problem-solving, accountability.\\n')]\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "turbo_llm_memory = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "\n",
    "memory_llm_conversation = ConversationChain(\n",
    "    llm=turbo_llm_memory, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "messages = prompt_template.format_messages(text=documents[6].page_content)\n",
    "# chat = ChatOpenAI(temperature=0.0, model=turbo_llm_memory)\n",
    "response = memory_llm_conversation(messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:26:43.109418700Z",
     "start_time": "2024-02-29T18:26:37.374132600Z"
    }
   },
   "id": "69ec60d269e52552"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a20d8a218bb0e0c0"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', 'history', 'response'])\n"
     ]
    }
   ],
   "source": [
    "type(response)\n",
    "\n",
    "print(response.keys())\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:26:44.944449400Z",
     "start_time": "2024-02-29T18:26:44.923372200Z"
    }
   },
   "id": "cda8fd454de201ce"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills: Python, R-programming, MATLAB, SQL, C, C++, C#, HTML, CSS, Java, XML, AWS, Azure, Google Cloud, Snowflake, Informatica, SAP SAC, JIRA, Azure DevOps, SharePoint, MS-Teams, Text comprehension, NLP, Classification, Pattern Recognition, Imaging Recognition and Detection, forecasting, clustering, Sentiment Analysis, Predictive Analytics, Decision Analytics, Association Analysis, Attribution modeling, Classification and Regression Trees (CART), SVM, Random Forest, GBM, PCA, RNN, Regression, Naïve Bayes, Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling, NumPy, Pandas, Scikit-learn, TensorFlow, SciPy, Matplotlib, Seaborn, Keras, NLTK, Beautiful Soup, StatsModels, PyTorch, OpenCV, Sweetviz, Theano, Bokeh, Plotly, Genism, Scrapy, kivy, Caffe2, Jupyter Notebook, Spyder, RStudio, Visual Studio, SharePoint, A/B testing GitHub, Git, Oracle, MS SQL Server, MS Access, Amazon S3, Azure, Mongo DB, My SQL, PostgreSQL, Hadoop, Pyspark, Communication and presentation skills, teamwork, leadership, critical thinking, creativity, problem-solving, accountability.\n"
     ]
    }
   ],
   "source": [
    "print(response['response'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:26:46.079476Z",
     "start_time": "2024-02-29T18:26:46.065473200Z"
    }
   },
   "id": "a0dbc9393252f007"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8126cc52657309b9"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template=\"For the following text, extract the following information:\\n\\nWarning: Don't greet or write any introduction. Just start with the answer to the prompts.Do as per the instructions given in the prompt.If you don't know the answer,leave that part (keep blank) and move to the next part \\n\\nName and contact: Extract the name and contact details of the candidate. \\nSkills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\nEducation: What is the highest education of the candidate and what is the GPA as mentioned in the text?Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\\n\\n\\nWork experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\\n\\nImportant:\\nFormat the output as JSON with the following keys:\\nSkills\\nEducation\\nWork experience\\n\\ntext: {text}\\n\"))]\n"
     ]
    }
   ],
   "source": [
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Warning: Don't greet or write any introduction. Just start with the answer to the prompts.Do as per the instructions given in the prompt.If you don't know the answer,leave that part (keep blank) and move to the next part \n",
    "\n",
    "Name and contact: Extract the name and contact details of the candidate. \\\n",
    "\n",
    "Skills: what are the technical and non technical skills? \\\n",
    "Answer output them as a comma separated Python list.\n",
    "\n",
    "Education: What is the highest education of the candidate and what is the GPA as mentioned in the text?\\\n",
    "Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\n",
    "\n",
    "\n",
    "Work experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designation\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Important:\n",
    "Format the output as JSON with the following keys:\n",
    "Skills\n",
    "Education\n",
    "Work experience\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:31:08.910916400Z",
     "start_time": "2024-02-29T18:31:08.886833100Z"
    }
   },
   "id": "811f1d4afc30e724"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: [HumanMessage(content=\"For the following text, extract the following information:\\n\\nWarning: Don't greet or write any introduction. Just start with the answer to the prompts.Do as per the instructions given in the prompt.If you don't know the answer,leave that part (keep blank) and move to the next part \\n\\nName and contact: Extract the name and contact details of the candidate. \\nSkills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\nEducation: What is the highest education of the candidate and what is the GPA as mentioned in the text?Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\\n\\n\\nWork experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\\n\\nImportant:\\nFormat the output as JSON with the following keys:\\nSkills\\nEducation\\nWork experience\\n\\ntext: Maaz Muhammad Khan\\n\\n\\uf020maazkhan1211@gmail.com | (216)-666-095|\\n\\nEducation\\n\\nM.S. Computer Science (GPA 3.77) January 2020 –December 2021 Cleveland State University\\n\\nRelevant Coursework: Data Mining, Big data, Machine Learning, Artificial Intelligence, Data communication and Networks\\n\\nB.S. Computer Science (GPA 3.39) January 2015 – December 2018\\n\\nMuhammad Ali Jinnah University\\n\\nRelevant Coursework: Database Systems, Cloud Computing, Statistics, Algorithms and Data Structures, Object Oriented Programming, Human Computer Interaction, Web Development, Computer Architecture, Software Requirement Engineering, Software Quality Assurance\\n\\nWork Experience\\n\\nBusiness Intelligence Analyst\\\\Data Engineer Jan 2022-Present New York Air Brake, Watertown, NY • Designed & developed visual BI reports and dashboards to support management decision-making using Power bi and Tableau. • Developed and automated data pipelines and ETL process and deployed new data models to make datasets readily-consumable by visualization tools\\n\\nusing Snowflake and SQL Server.\\n\\nDrove efficiencies throughout the organization by automating business processes. • Gathered and analyzed data from multiple sources to solve complex problems and prepared summaries of analysis and findings to management team. • Worked with leadership to understand business needs and translate those into reporting solutions. • • Developed and maintain data quality standards and data governance policies. Systems Administrator May 2019 – Jan 2022 Lansing Board of Water and Light, Lansing, MI • Designed and developed dashboards which helped managers in making critical decisions to drive employee productivity and resource utilization. • Created dashboards with context and overlayed analytics and live, interactive, reports that easily compare and analyze assets using Microsoft Power-Bi • Managed server, data repositories and the real-time data storage, normalization, analytics, and notification engine using Microsoft Azure SQL database. • Summarized and filtered years of historical data and view alongside real-time data which allowed summary calculations • Used python and SQL for ETL processes data cleansing, normalization, and transformation for operations analytics project. • Also used Microsoft Azure DevOps for version control, reporting, requirements management, project management, automated builds, testing and release\\n\\nmanagement capabilities for the entire application lifecycle(Agile Methodology).\\n\\nWorked on real-time data from grid and network devices for data analytics and applied machine learning to automate network response. Research Assistant Feb 2020 – April 2021 Artificial Intelligence and Computer Vision Lab, Cleveland State University, Cleveland, OH. • • • • • • Applied machine learning models like, CNN, KNN, Kmeans, SVM, Naïve Bayes, Regression, Gradient Boosting Algorithms • Determined the hidden relation between images and text using text and image mining libraries Nltk and Gensim • Worked on image recognition using Tensorflow and Keras for implementation of (CNN) with different initializers like Adam and SGD. Data analyst/Enginer May 2016 – July 2020 John Carroll University, Cleveland, OH • Assisted data scientists in building sentiment analysis and Used python and SQL for text mining and artificial intelligence techniques. • • Analyzed complex data sets and translated data into insights to drive key business decisions using Tableau and Microsoft Power BI. • • Mined huge textual dataset of online reviews of different movies using NLTk and Genism and Categorized data into POS, removed stop words • BI tools: Tableau, Microsoft Power BI, Power Query, DAX SAP Analytics Looker, Google Charts, Qlik, SSRS, SSIS\\n\\nWorked on real-time data from grid and network devices for data analytics and applied machine learning to automate network response. Research Assistant Feb 2020 – April 2021 Artificial Intelligence and Computer Vision Lab, Cleveland State University, Cleveland, OH. • • • • • • Applied machine learning models like, CNN, KNN, Kmeans, SVM, Naïve Bayes, Regression, Gradient Boosting Algorithms • Determined the hidden relation between images and text using text and image mining libraries Nltk and Gensim • Worked on image recognition using Tensorflow and Keras for implementation of (CNN) with different initializers like Adam and SGD. Data analyst/Enginer May 2016 – July 2020 John Carroll University, Cleveland, OH • Assisted data scientists in building sentiment analysis and Used python and SQL for text mining and artificial intelligence techniques. • • Analyzed complex data sets and translated data into insights to drive key business decisions using Tableau and Microsoft Power BI. • • Mined huge textual dataset of online reviews of different movies using NLTk and Genism and Categorized data into POS, removed stop words • BI tools: Tableau, Microsoft Power BI, Power Query, DAX SAP Analytics Looker, Google Charts, Qlik, SSRS, SSIS\\n\\nCollected dataset by web scraping techniques using libraries like selenium, beautiful soup, and response. Stored unstructured using Pymongo and did preprocessing/cleansing using Sk.learn and Opencv, numpy, pandas , skiamge Sliced data using Sklearn.model_selection for testing and training and added layers filters and activation to convolutional neural networks. Transformed data and developed a system which classifies images into subcategories with the help of cnn using Pytorch & Torchvision Extracted features from datasets using ICA, LBP, Hog, PCA , LDA, LLE, AE , and etc.\\n\\nAnalytic Development: Python, R-programming, MATLAB, SQL, C, C++, C#, HTML, CSS, Java, XML • • Cloud Services: AWS, Azure and Google Cloud, Snowflake, Informatica • Miscellaneous Tools: SAP SAC, JIRA, Azure DevOps, SharePoint MS-Teams. • Artificial Intelligence & Analysis Methods:: Text comprehension, NLP, Classification, Pattern Recognition, Imaging Recognition and Detection, forecasting, clustering, Sentiment Analysis, Predictive Analytics, Decision Analytics, Association Analysis, Attribution modeling, Classification and Regression Trees (CART), SVM, Random Forest, GBM,, PCA, RNN, Regression, Naïve Bayes\\n\\nData Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling • Python Packages: NumPy, Pandas, Scikit-learn, TensorFlow, SciPy, Matplotlib, Seaborn, Keras, NLTK, Beautiful Soup, SciPy, StatsModels, PyTorch, OpenCV, Sweetviz, Theano, Bokeh, Plotly, Genism, Scrapy, kivy, Caffe2 • IDE & Version Control: Jupyter Notebook, Spyder, RStudio, Visual Studio, SharePoint, A/B testing GitHub, Git, Agile methodologies • Databases: Oracle, MS SQL Server, Snowflake, MS Access Amazon S3, Azure, Mongo DB , My SQL, PostgreSQL and Hadoop, Pyspark . • Soft Skills: Communication and presentation skills; teamwork; leadership, critical thinking, creativity, and problem-solving, accountability.\\n\")]\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "turbo_llm_memory = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "\n",
    "memory_llm_conversation = ConversationChain(\n",
    "    llm=turbo_llm_memory, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "messages = prompt_template.format_messages(text=documents[6].page_content)\n",
    "# chat = ChatOpenAI(temperature=0.0, model=turbo_llm_memory)\n",
    "response = memory_llm_conversation(messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:31:25.315922300Z",
     "start_time": "2024-02-29T18:31:10.682694300Z"
    }
   },
   "id": "cc2e955c37c2bf8e"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Skills\": [\n",
      "        \"Python\",\n",
      "        \"R-programming\",\n",
      "        \"MATLAB\",\n",
      "        \"SQL\",\n",
      "        \"C\",\n",
      "        \"C++\",\n",
      "        \"C#\",\n",
      "        \"HTML\",\n",
      "        \"CSS\",\n",
      "        \"Java\",\n",
      "        \"XML\",\n",
      "        \"AWS\",\n",
      "        \"Azure\",\n",
      "        \"Google Cloud\",\n",
      "        \"Snowflake\",\n",
      "        \"Informatica\",\n",
      "        \"SAP SAC\",\n",
      "        \"JIRA\",\n",
      "        \"Azure DevOps\",\n",
      "        \"SharePoint\",\n",
      "        \"MS-Teams\",\n",
      "        \"Text comprehension\",\n",
      "        \"NLP\",\n",
      "        \"Classification\",\n",
      "        \"Pattern Recognition\",\n",
      "        \"Imaging Recognition and Detection\",\n",
      "        \"forecasting\",\n",
      "        \"clustering\",\n",
      "        \"Sentiment Analysis\",\n",
      "        \"Predictive Analytics\",\n",
      "        \"Decision Analytics\",\n",
      "        \"Association Analysis\",\n",
      "        \"Attribution modeling\",\n",
      "        \"Classification and Regression Trees (CART)\",\n",
      "        \"SVM\",\n",
      "        \"Random Forest\",\n",
      "        \"GBM\",\n",
      "        \"PCA\",\n",
      "        \"RNN\",\n",
      "        \"Regression\",\n",
      "        \"Naïve Bayes\",\n",
      "        \"Bayesian Analysis\",\n",
      "        \"Statistical Inference\",\n",
      "        \"Predictive Modeling\",\n",
      "        \"Stochastic Modeling\",\n",
      "        \"Linear Modeling\",\n",
      "        \"Behavioral Modeling\",\n",
      "        \"NumPy\",\n",
      "        \"Pandas\",\n",
      "        \"Scikit-learn\",\n",
      "        \"TensorFlow\",\n",
      "        \"SciPy\",\n",
      "        \"Matplotlib\",\n",
      "        \"Seaborn\",\n",
      "        \"Keras\",\n",
      "        \"NLTK\",\n",
      "        \"Beautiful Soup\",\n",
      "        \"StatsModels\",\n",
      "        \"PyTorch\",\n",
      "        \"OpenCV\",\n",
      "        \"Sweetviz\",\n",
      "        \"Theano\",\n",
      "        \"Bokeh\",\n",
      "        \"Plotly\",\n",
      "        \"Genism\",\n",
      "        \"Scrapy\",\n",
      "        \"kivy\",\n",
      "        \"Caffe2\",\n",
      "        \"Jupyter Notebook\",\n",
      "        \"Spyder\",\n",
      "        \"RStudio\",\n",
      "        \"Visual Studio\",\n",
      "        \"A/B testing\",\n",
      "        \"GitHub\",\n",
      "        \"Git\",\n",
      "        \"Oracle\",\n",
      "        \"MS SQL Server\",\n",
      "        \"MS Access\",\n",
      "        \"Amazon S3\",\n",
      "        \"Azure\",\n",
      "        \"Mongo DB\",\n",
      "        \"My SQL\",\n",
      "        \"PostgreSQL\",\n",
      "        \"Hadoop\",\n",
      "        \"Pyspark\"\n",
      "    ],\n",
      "    \"Education\": [\n",
      "        \"Cleveland State University\",\n",
      "        \"3.77\",\n",
      "        \"Muhammad Ali Jinnah University\",\n",
      "        \"3.39\"\n",
      "    ],\n",
      "    \"Work experience\": [\n",
      "        \"New York Air Brake, Watertown, NY, Business Intelligence Analyst\\\\Data Engineer, Jan 2022-Present\",\n",
      "        \"Lansing Board of Water and Light, Lansing, MI, Systems Administrator, May 2019 – Jan 2022\",\n",
      "        \"Artificial Intelligence and Computer Vision Lab, Cleveland State University, Cleveland, OH, Research Assistant, Feb 2020 – April 2021\",\n",
      "        \"John Carroll University, Cleveland, OH, Data analyst/Enginer, May 2016 – July 2020\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response['response'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:31:27.496873100Z",
     "start_time": "2024-02-29T18:31:27.489869400Z"
    }
   },
   "id": "56f8e0ac6563754"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8f1f509c696f8305"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
